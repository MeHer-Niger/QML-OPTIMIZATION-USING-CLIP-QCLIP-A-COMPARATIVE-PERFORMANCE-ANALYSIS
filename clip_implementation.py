# -*- coding: utf-8 -*-
"""CLIP_implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_RW7Ytzi_GhSgAC1H5kmX6VGyt-hjWo
"""

!pip install torch transformers dataset

pip install -U torch datasets transformers

from datasets import load_dataset

imagenette = load_dataset(
    'food101',
    split='train',
    ignore_verifications=False  # set to True if seeing splits Error
)
imagenette

from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel
import torch

# if you have CUDA or MPS, set it to the active device like this
device = "cuda" if torch.cuda.is_available() else \
         ("mps" if torch.backends.mps.is_available() else "cpu")
model_id = "openai/clip-vit-base-patch32"

# we initialize a tokenizer, image processor, and the model itself
tokenizer = CLIPTokenizerFast.from_pretrained(model_id)
processor = CLIPProcessor.from_pretrained(model_id)
model = CLIPModel.from_pretrained(model_id).to(device)

import numpy as np

np.random.seed(0)
# select 100 random image index values
sample_idx = np.random.randint(0, len(imagenette)+1, 5000).tolist()
# extract the image sample from the dataset
images = [imagenette[i]['image'] for i in sample_idx]

from tqdm.auto import tqdm

batch_size = 16
image_arr = None

for i in tqdm(range(0, len(images), batch_size)):
    # select batch of images
    batch = images[i:i+batch_size]
    # process and resize
    batch = processor(
        text=None,
        images=batch,
        return_tensors='pt',
        padding=True
    )['pixel_values'].to(device)
    # get image embeddings
    batch_emb = model.get_image_features(pixel_values=batch)
    # convert to numpy array
    batch_emb = batch_emb.squeeze(0)
    batch_emb = batch_emb.cpu().detach().numpy()
    # add to larger array of all image embeddings
    if image_arr is None:
        image_arr = batch_emb
    else:
        image_arr = np.concatenate((image_arr, batch_emb), axis=0)
image_arr.shape

image_arr = image_arr / np.linalg.norm(image_arr, axis=0)
image_arr.min(), image_arr.max()

final_result = []

from PIL import Image
import requests
url = 'https://images.unsplash.com/photo-1600028068383-ea11a7a101f3?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8Mnx8UGl6emElMjBNYXJnaGVyaXRhfGVufDB8fDB8fHww'
uploaded_image = Image.open(requests.get(url, stream=True).raw)
uploaded_image

img1 = processor(
    text=None,
    images=uploaded_image,
    return_tensors='pt'
)['pixel_values'].to(device)
img1.shape

img_emb2 = model.get_image_features(img1)

img_emb2 = img_emb2.cpu().detach().numpy()
scores = np.dot(img_emb2, image_arr.T)
scores.shape

top_k = 50
# get the top k indices for most similar vecs
idx = np.argsort(-scores[0])[:top_k]
idx

label_arr = []

def find_label(idx,imagenette):
  for j in range(len(idx)):
    for i in range(len(imagenette)):
        # Check if the current entry's image path matches the target image path
        if imagenette[i]['image'] == images[idx[j]]:
            label_arr.append(imagenette[i]['label'])
            break

# Find the label for the given image
find_label(idx, imagenette)
label_arr

from collections import Counter

def max_occurrences(arr):
    # Use Counter to count occurrences of each element in the array
    count_dict = Counter(arr)

    # Find the maximum occurrence
    max_occurrence = max(count_dict.values())

    return max_occurrence

(max_occurrences(label_arr)/50)*100

import matplotlib.pyplot as plt

# display the results
for i in idx:
    print(f"{i}: {images[i]}")
    plt.imshow(images[i], cmap='gray')
    plt.show()

final_result

result=[element * 10 for element in final_result]

result

import matplotlib.pyplot as plt
import numpy as np

# Your array
data = result

x_values = np.arange(len(data)) * 5

# Create a line plot with modified x-axis values
plt.plot(x_values,data, marker='o', linestyle='-')

# Set labels and title
plt.xlabel('No. of test images')
plt.ylabel('Accuracy')

# Show the plot
plt.show()

label_array=[]

def find_label(image,imagenette):
  for j in range(len(idx))
    for i in range(len(imagenette)):
        # Check if the current entry's image path matches the target image path
        if imagenette[i]['image'] == image:
            # Return the label associated with the image
            return imagenette[i]['label']

# Find the label for the given image
image_label = find_label(images[idx[0]], imagenette)
image_label

if image_label is not None:
    print("The label of the image in the Imagenette dataset is:", image_label)
else:
    print("Image not found in the Imagenette dataset.")

for i in range(10):
    print(f"{i}: {imagenette[i]['label']}")