# QML-OPTIMIZATION-USING-CLIP-QCLIP-A-COMPARATIVE-PERFORMANCE-ANALYSIS

# ğŸ§  CLIP & Quantum CLIP (qCLIP) Implementations

This repository contains two implementations of the CLIP model â€” one classical and one quantum-enhanced â€” for image-text similarity matching and classification using the **Food101** dataset. The goal is to evaluate how quantum circuits can enhance or complement traditional deep learning models.

---

## ğŸ“ Project Structure


---

## ğŸ” Overview

| Model  | Description |
|--------|-------------|
| **CLIP**   | Uses OpenAI's `clip-vit-base-patch32` to encode image features from Food101 and perform similarity matching based on cosine scores. |
| **qCLIP**  | Enhances the classical pipeline with a PennyLane-based quantum circuit using simulated qubits. Processes an imageâ€™s grayscale pixel intensities as quantum inputs and optimizes them for embedding. |

---

## ğŸ§ª Technologies Used

- ğŸ¤— Transformers (CLIP Model)
- Torch (PyTorch)
- Datasets (HuggingFace)
- PennyLane & PennyLane-Qiskit (Quantum processing)
- NumPy, Matplotlib
- Google Colab

---

## ğŸ“¦ Installation

Install dependencies:
```bash
pip install torch transformers datasets matplotlib
pip install pennylane pennylane-qiskit  # For qCLIP only


